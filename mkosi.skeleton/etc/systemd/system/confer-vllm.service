[Unit]
Description=Confer vLLM Service
After=local-fs.target mnt-config.mount mnt-models.mount nvidia-persistenced.service nvidia-fabricmanager.service nvidia-cc-attestation.service
Wants=mnt-models.mount
Requires=mnt-config.mount nvidia-fabricmanager.service nvidia-cc-attestation.service
Before=confer-proxy.service

[Service]
Type=simple
User=root
RuntimeDirectory=vllm
WorkingDirectory=/run/vllm
EnvironmentFile=/mnt/config/secrets.env
ExecStartPre=/bin/mkdir -p /tmp/huggingface /tmp/.cache
ExecStart=/usr/bin/python3 -m vllm.entrypoints.openai.api_server \
    --model ${VLLM_MODEL} \
    --served-model-name ${VLLM_SERVED_MODEL_NAME} \
    --host ${VLLM_HOST} \
    --port ${VLLM_PORT} \
    --gpu-memory-utilization ${VLLM_GPU_MEMORY} \
    --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE} \
    --enable-expert-parallel \
    --enable-auto-tool-choice \
    --tool-call-parser ${VLLM_TOOL_CALL_PARSER} \
    --max-model-len ${VLLM_MAX_MODEL_LEN} \
    --max-num-seqs ${VLLM_MAX_NUM_SEQS} \
    --max-num-batched-tokens ${VLLM_MAX_NUM_BATCHED_TOKENS} \
    --enable-chunked-prefill \
    --enable-prefix-caching
Restart=always
RestartSec=10
Environment="HF_HUB_CACHE=/tmp/huggingface"
Environment="HOME=/tmp"
Environment="XDG_CACHE_HOME=/tmp/.cache"

[Install]
WantedBy=multi-user.target
